Welcome back, engineers. Indy Dev Dan here. Well, we were right. Prompt chaining, aka chain of thought, is the key for improved model results. On the channel, we've released three big prompt chaining videos over the past year, emphasizing the importance and the power that prompt chaining gives your tools, products, and ultimately your users. This pattern was so spot on that the Gen AI leader, OpenAI, embedded the pattern into their new o1 series. With this release, OpenAI reset the counter because it's such a powerful difference in the way the model performs. By now, you've seen all the clickbaity high-level videos. You know what this is about. And if you're a subscriber to the channel, you know what we're about here. Let's really dive in to the value proposition of the new o1 series. In this video, we're going to walk through three viable examples showing how exactly to use these new reasoning models. Let me be super straightforward with you. This model is not easy to use properly. As OpenAI explicitly mentions, the array of prompt engineering abilities you and I have been building up don't all apply here. We'll be using two tools from one of my favorite engineers in the Gen AI space and we're going to work through these examples so you can know exactly when it's time to double-click into the new O-1 reasoning models so you can maximize the value they give you. Content generation is a killer obvious use case of large language models. One more difficult, interesting example of content generation is generating YouTube chapters. You can see here on our previous AI coding video, we have these chapters generated, and they have this simple form, minutes, seconds, title. This problem is simple to understand, but harder to implement. So let's use chapter generation to show off a concrete example of o1 beating out the previous state-of-the-art model, Claude 3.5. Let's look at a concrete example of what this looks like. So I have the transcript for that YouTube video right here, and it looks like this, right? I also have a prompt here that generates the YouTube chapters based on the transcription with a time stamp. So you can see here I have the entire transcript pasted here and this is a block in this prompt. So this is a great use case for LLMs, but this is also a use case where LMs typically falls short. So let's go ahead and use a tool from one of my favorite engineers, Simon W. He's built a CLI-based LLM tool that lets you run prompts directly in your terminal. So if I type LLM-M-O-1 Mini Ping, and we'll just get a simple response back for ping. So nothing special happening here. I'll link this library in the description. The key value proposition of this library is that it allows us to do something like this. So if we see the into YouTube chapters and focus in on this specific prompt. So we can see we have the transcript and we have the prompt here lm dash m clod 3.5 sonnet redirect this prompt into the lm so now we're just going to run clot 3.5 sonnet with this entire prompt so let's just go ahead and fire this off. Cloud 3.5 Sonnet is running through this. It's generating results, generating chapters for us, based on this prompt here, right? And you can see some of the instructions, right? We're just being detailed. Time stamps are in minute seconds. Use the example output. So we have a couple examples right here. And then we have some SEO keywords we're trying to hit. And then we have a couple additional instructions. I can tell you that Cloud 3.5 Sonnet performs OK here. It's not the best. If you watch that video, you know, we don't actually reveal the secret sauce of AI coding at the beginning of the video. It's something like it's minute five or I think minute 14. This is good. What we can do here with this tool is redirect this output to a new text file. So I'll just say YouTube chapter sonnet results.t.t. Run the prompt again. Generate the output saved to this text file. So let's go ahead and run the brand new o1 reasoning models. I'll just hit up and then I'll make a couple tweaks here, right? I'm going to do the exact same thing, except we want to run the top of the line, o1 preview model. And then we're just going to update the name of the output file. o1 preview results. Okay. So we're going to fire that off. And let's go ahead and take a look at our Sonnet results. Right. So you can see here we got this clean chapter generation here. And now let's go ahead and get the results for o1 preview. So this is going to take some time. I'm going to go ahead and cut this part out. All right. So we've got a result. Let's go ahead and take a look side by side. So here's the o1 results and here's the Sonnet 3.5 results. So we still have the unveiling secret sauce right at 0029. But we do also get the correct timestamp at about 1354, right? So revealing the secret sauce effective coding patterns. We can see that that pattern is revealed there. You know, we have some more options here. We can see that Sonic gave us fewer chapters, whereas o1 gave us quite a few more. I'm curious what you think about this. Do you prefer more chapters or fewer? It's easier to trim chapters down than to add chapters. But if we just look through here, I would come in, I would delete this item here. I do like the mentioning of Ader and cursor. There's a really important block here, SEO keywords to hit. And we can just kind of go through and make sure we can see everything that's matched. We can say AI Devlog, AI coding devlog, that's there. We can see Andy Dev Dan, that's there. What else do we have here? We have, of course, Ader Cursor, A. Devlog. And then we have, you know, secret sauce that does get mentioned. So, you know, that's good there. If we search this on the Sonnet 3.5 side, you'll see something really interesting, right? We're actually missing a lot of our keywords. Even if you, you know, drop this down to 12 lines just to match of this Regic search that we're doing here. Sonnet actually missed out a ton on our SEO keywords that we're asking for in our prompt. So this is a pretty big deal and it hints at one of the key value propositions. When you're wondering if you should pull up the big guns and use the new o1 reasoning model series, one of the questions you should ask yourself is, do you need the model to follow your instructions to a T? Is precision and accuracy important for your prompts? If I had to explain the value proposition of the o1 reasoning models in one sentence it would be the o1 reasoning models are profoundly exceptional at instruction following and iterating on proposed solutions based on your instructions and of course we know why it's better it's because it's thinking over and over it's creating drafts and then it's outputting the results after thinking about the solution and iterating on it. So this is one example where we can see although Claw 3.5 Sonnet performs well enough, you can simply swap out the model in this case and you'll get a lot more performance and instruction following out of the new o1 reasoning models. We can also dial directly into the transcript and just search to verify, right? So we have the secret sauce revealed. If we go ahead and search this timestamp, you can see that it's following the instructions really, really well. At the 1354 minute mark, this is where I explicitly say this is the secret sauce of AI coding, right? Our code is automatically getting validated for us. This is the secret sauce of AI coding. I've run o1 versus Sonnet 3.5 in many cases and the typical trend is O1 models, although they really make you appreciate how fast Claude 3.5 and GPT40 Mini are, the o1 models will outclass most of your previous prompts, especially the ones with more detail. So let's look at another really interesting use case for these models. There are some things that are just too hard to do for the current state of the art. Let's look at an example related to AI coding. So let's go ahead and crack open this prompt here. We have this prompt here that given a diff automatically looks for bugs and provides solutions to an AI coding assistant LLM to automatically fix the code. There's a lot of value here in this prompt. If we look at the output format, you can see that we're specifying in TypeScript using interfaces, the AI assistant fix, so we want a list of fixes. And then you can see here, and we can go ahead and just pull these out into a quick type script format so it's easier to read right so you can see here that we have oh that's actually a good catch here I didn't see that so you can see here that we have a simple structure we want the file we want the starting line and, we want the starting line, and then we want the AI coding resolution prompt. So this is really important. This is getting kind of meta, right? We have a prompt here that is going to run on your code, on your diff, and then it's going to tell the LM how to fix the problem, right? We want a description of the bug and we want a severity, right? And that's going to be the output format for this prompt. So what we need to do is get a diff of whatever code changes that we want checked. And then we want the code files. So let's look at the code files first. This is this really cool tool from Simon W. And he has this tool that converts files to prompts. And he just added this really great flag CXML. So this is a cloud XML format. This converts your files from a directory that also respects your, you know, ignore and your hidden files. And it converts that into a prompt that you can use inside of prompts right so let's go ahead i'm just going to copy this and all i'm going to do is open up the terminal here i'm going to paste this in i'm going to change the directory so i just want it in this you know i want every file here copy that to the clipboard, right? So we'll just say cop. It's going to skip the bond lock file. That's totally fine. And now I'm just going to add this as a file in this code base in API wrapper code bundle. I'll just call that dot txt and then we'll just paste this. And actually we can do a .xml file. Okay, so you can see here this tool is really powerful. What it's done here is it's gone through and it's pulled all the files in this code base. And you can see here we have our Notion Utils. We have our modules, notion.ts, constants, and you see, right, those are the contents we had in the previous video. And it's basically pulled all of our content into a single file, right? So we can save this, copy all this, go back to our prompt and paste this in our code files, right? So I'll just go ahead and paste that. And what i'll actually do is before i paste i'll use a c data block just to prevent any formatting from happening so i'll save that and let's go back up to code files collapse that now let's get the diff right so i'll just create a simple diff um we'll say get diff uh head one. And we'll just copy that. And you can see that's just one commit backward. So I'll just pace that diff in. And so this is just all the changes made. And now what we have is a, let me actually wrap this in the C data as well. So now what we have is a diff of our current co-changes and all the related files. Right. So you can see here 22,000 token prompt, larger, medium-sized prompt. And what we can do now is open up our terminal, check our files, and let's just go ahead and run this prompt, right? This is one of the prompts that is much harder for the previous generation models to run. So I'm just going to go ahead and go right for gold. So I'll say LLM-M-Th, top of the line, O1 preview, and then once again, we're going to redirect the file of our AI coding, diff prompt. And let's go ahead and get the solution redirected into a AI coding o1 preview JSON, right? Because we're getting a JSON file back here. So you can see that file got generated there. So while we're waiting for this to complete, just want to, you know, mention the prompt format we're using here. We're using the classic XML prompt format. This is really powerful. We did a video, I'll link in the description, where we created some benchmarks to see what is the best prompt format. This is one of the things that the OpenAI team explicitly mentions in their reasoning page to be valuable when using the new reasoning models, right? So it says use delimiters for clarity, triple quotes, XML tags, section titles. So this is one of our learnings from our prompt engineering journey on the channel that can definitely stick around. I don't see this going anywhere over the long haul. So this is a really important discovery, really important pattern to separate portions of your prompt using specifically XML. And we'll put out a video in the future validating that for the reasoning models, we're going to get about the same results using the XML tags versus markdown format or just plain text format, right? So let's go ahead and look at our preview model. So that's completed. Let's go ahead and look at the results, right? So what we should see here is a, you know, JSON file, which is good. We did get the markdown syntax there. That's fine. But this is really cool, right? So what you can see here is a list of fixes and the AI coding resolution prompt, right? We now have a highly effective reasoning model looking at our code, looking at the diff. It's telling us the exact issue of our code base, right? So we're getting a nice kind of clean code review from our language model. But it's also giving us, let's go ahead and do a multi-line wrap. We're also getting the solution to pass back into our AI coding assistant, right? So whether it's ater, cursor, continue, whatever you're using, you can use a prompt like this with the o1 reasoning series to work through a bunch of code. Remember, we just, you know, we just ran this prompt on 20K tokens at the o1 preview prices. You know, hit the like, hit this up, support the channel. We are definitely letting some money on fire here, but it is well worth it to show you the value of these models. Again, we have the output format. We're specifying TypeScript interfaces. We're getting really detailed here with what we want the output to look like. We're placing all of our code files, right? So this could be a lot larger. This is kind of an anti-pattern because opening eye does explicitly mention, you know, you do want to avoid RAG as much as possible. So pulling in a bunch of content. But at the same time, this is super relevant for the model to perform. So, you know, we're kind of on the line there. But we have our code files, thanks to Simon W's files to prompt library. And then we also have just a raw diff, right? So we're just using get diff here. And then we have some detailed instructions on how things work. That's the secret. It's also the prompt. And then of course, just the high level purpose. So that gives us this great result thanks to to the O1 preview model. So if this is all making sense and you're understanding how and when to use the O1 series, drop the like, drop the sub, show your appreciation, it helps the channel grow, and it helps other engineers like you find this content. So you can kind of work through these things. Readability, This doesn't matter who cares, minor, who cares, mine who cares. But it did detect a major bug for us, right? And it said that imports should be placed at the top of the file. This can cause issues with module loading. Definitely if your imports are in the wrong place, there are some issues that can arise. So if you go ahead and look at this file here and this code base, we just quickly search this, and we just look for imports. Yeah, so we can see this import is happening in a wrong location. It's really interesting to see o1 giving us high fidelity answers like this on our code. So this kind of hits at that same reoccurring theme. The O1 series is really, really great at following your instructions. Why? Because it's iterating over potential answers. And you've seen this inside of chat TPT, right? If we just let it fire off here, it's going to think think and all the value here is in the fact that it's looking over things it's thinking things through step by step and it's iterating over potential results right this is the whole thing step by step effectively you know prompt chaining inside the model it's really really cool to see the actual thinking portion, although, as we know, some of the reasoning is actually hidden on the Open AI servers. So this is really cool. This is going to keep running and, you know, create some result. We don't have to wait for that. Let's continue. All right, so let's focus in on our last use case here. So let's take in on our last use case here. So let's take a look at a sentiment analysis prompt. Let me show you the purpose of this prompt here. And then we'll change the language mode to XML and we'll collapse. Analyze the aggregate sentiment of a list of comments from Hacker News, right? So this is a Hacker News sentiment analysis. And we're actually looking at this post here, which, you know, directly discuss Open AI's new O-1 chain of thought models. Again, big shout out Simon W. He's the author of the linked content. And what we're going to do is just do a sentiment analysis on this post. This prompt is likely an entire product in itself. But what we're going to do is just do a sentiment analysis on this post. This prompt is likely an entire product in itself. But what we're going to do here is lean on the reasoning iterative capabilities of the o1 preview model. We're going to create three segments of sentiment analysis. And let me just show you what the response format looks like. Right. So we have this kind of wild two format sentiment. Right. So we again pull these into separate kind of wild two format sentiment, right? So we again, pull these into separate files. We have both the markdown format of this analysis. And then we have the aggregate sentiment, right? So we have this JSON structure. You can see here we have positive, negative, and nuanced. And so this is really interesting, right? We have an entire sentiment analysis for more complex, positive, negative, or neither interesting, right? We have an entire sentiment analysis for more complex, positive, negative, or neither content, right? And if we open this up, we can see we have themes, a list of themes. We have an idea that describes the nuanced sentiment. We have the number of times themes like this was hinted at or mentioned. And then we have a couple standout comments. So let's pull down this post. We're going to use a tool called Algolia. And they have the responses here essentially cached. We have all this JSON. We're going to go ahead and just copy this down. And we're going to paste this here. This is going to be HN1 model discussion. And we're going to paste this here. This is going to be HN1 model discussion. And we're going to make this a JSON file, paste that in. And you can see here we have a hundred K, we have 130,000 tokens. So I have a massive prompt here, 130K tokens. Let's go ahead and dial this prompt down a little bit. We're going to use a classic tool jq we're going to run jq jq dot on h n model discussion so that's the entire thing and now what we want to do is dial this down right so let's just get the children and so you know children contains all the comments so what we want to do here is just get children and let's get the first three children, right? Let's see what that looks like. And let's go ahead and output that to a new file. We'll say HN-O-1, itter, children.json. So we have that new file. It's just the children and you can see there we're down to 10 kit tokens. So much more manageable let's go ahead and just work on a you know smaller segmented version of this and let's operate on these 10k tokens right so if we line collapse we can see we're still getting a lot of decent comments here a lot of good stuff happening right so let's go ahead and run our sentiment analysis prompt on this file if we go ahead close this go back to our prompt here. You can see that we have hin.json. So we're going to pass in our JSON here, paste in our new set of results here. Right. So now we have all those comments. If we open up the instructions here, we can kind of reveal some of the secret sauce here. Basically we're saying respond in this specific response format. I'm referring to the block there. And what we want to do is, you know, basically create the nuance sentiment and the, you know, positive and negative as well. And again, the great part about the o1 series is that it's going to run the prompt. It's going to iterate on it. And it's going to continue reflecting on the results based on my instructions. Right. So let's go and just fire this all using Simon W's LLM tool. So we have the prompt here. Let's go ahead and just kick that off. I'll say LM-LM-O-1 preview, left, direct our sentiment prompt. And let's go ahead and pipe the results out. Results, o1.Json. And this is going to give us a JSON file back. So we'll go ahead, let that prompt kick off. And that should generate a brand new sentiment analysis for us automatically and you know we can reuse this it's all about what you're passing into the hn.json and again you know one of the great parts about structuring your prompt like this and having them in text and using a great tool like lLM from the cly is that everything is reproducible. I have the static prompt that I can improve over time. I can run benchmarks against it. I can run a test library like prompt foo on this prompt and compare it to others. And we can know with certainty that this prompt is going to outperform other prompts. I'm really excited to, you know, share more about this model. This video's already getting really long. I'm going to try to edit it down a more about this model. This video is already getting really long. I'm going to try to edit it down a lot for you guys, but there are so many new things, so many new ideas to cover with the O1 reasoning models. The capabilities here are super off the charts. You know, drop the like, drop the stuff up if you're seeing a lot of the value that these models can give you. Part of the complexity in this prompt is that this response format is asking for a lot. I can guarantee you this is going to run for, you know, two, maybe three minutes based on all my work with this model so far. I'm asking for a lot here, right? Think about what this is asking. First off, do a sentiment analysis with both positive, negative, and nuance. Right. So this is new or it's a newer idea, right? Comments on each one of these, I probably could have pulled it out into a separate type, but, you know, I just placed it all here. And, you know, I want some standout comments that, you know, reflect the idea that creates the theme and it has to be positive. Right. So this is an action-packed prompt. Normally, I would break my own prompt chain to do segments of what this is doing. So not only am I asking for three perspectives with multiple themes, I'm also asking O one to give me an entire markdown version of this, right? So you can see here it's still spinning while I'm yapping off. But this is really, really incredible, right? So you can see here we have this instruction in the markdown response, respond with your results from you know this object type right but in human readable markdown format right so there's a lot going on in this prompt this is something that you just absolutely could not give to a previous generation model and expect great results out of but you know for my, from some benchmarking, I can guarantee you this is going to give us some really incredible results. And, you know, this is why I said in the beginning, it is hard to use the new O1 reasoning models properly. And what do I mean by properly? I mean, you can really push these models to do incredible things. And you can, you know, action pack your instructions and your rules and, you know, some of the content, right? Some of your XML or formatted blocks. You can really pack it in. And while it's working, while it's iterating, it's going to keep looking back at your prompt. And it's going to, you know, keep analyzing the results that it's iteratively, it's going to keep looking back at your prompt and it's going to, you know, keep analyzing the results that it's iteratively building up behind the scenes and check it against what you're asking. You know, there's a much simpler version of this prompt where it's already returned with the result, right? Because it's just simpler. I'm not asking a lot from it. But, you know, that's kind of a good way to phrase it. I'm not asking a lot from it. But, you know, that's that's kind of a good way to phrase it. You can ask a lot more from these reasoning models because again, they have some version of thinking step by step of reasoning, so and so forth. So there we don't. We just got our result. Let's go ahead and take a look. Okay. So here's our sentiment analysis, right? So first off, let's check the structure. Perfect, right? JSON markdown. Let's first browse through the JSON and just make sure we have all of our sections. So great, we have negative, nuance, and positive, right? So let's just dial into one of these. You can see here, we have our themes. So this is really great, right? We have, let's see how many themes we have here. Right, we have two concrete themes. Let's see what the theme is. Right. So there's a positive theme. Excitment about advancements and potential of GPT models, right, including GPT5. There's some really great comments in here. We can, you know, validate. I know some engineers, you might be one of them that are always worried about hallucinations you can easily double check this we just search right we have a great response here right and we can see that response is here in the hacker news thread that's great you know we have another one here i know sometimes sketch if a i can do this on 64k tokens iteratively full multle, I don't think I've actually been scared of super intelligence, singularly, until just now. Now this is AI, yeah, totally, right? Great take, completely agree. Right. So, you know, we have some positive standout comments with this theme. Let's go ahead and look at another positive theme, right? What was our other theme here, right? Anticipation on AI's impact. Totally. Yeah, that makes sense. So, you know, this user, the homie Eldid's uses, he's talking about the data shortage problem. This is true iterative development. These models are going to get deeply embedded into IDE's like cursor has. Right. So, you know, this is something that's happening right now. They already have this model. And, you know, you can already run it on cursor and aider. If we just search this, you know, you can just kind of see that there. We've already taken a look at that. And then we can dig into more nuance takes, right? So this is kind of cool, right? So we have positive negative and then we have nuance. And let's go ahead and format our markdown. This is pretty gnarly, right? I asked for a sentiment analysis with positive, nuance, and negative in JSON format. Then I asked for a markdown version of that in the same prompt, right? Let's go ahead and just take a look at this. Let's go ahead and move this to Markdown and let me go ahead and format this and just get that formatted. So this is really great, right? Let me go ahead and open up a preview and move that here. So, you know, we could almost just take this and post it to a blog or something, right? You know, we have our positive sentiments. Go ahead and look for our nuanced sentiment, right? So mixed feelings about hidden reasoning tokens with concerns about transparency and debugging. Okay, so, you know, that makes sense, right? As a developer, this is highly concerning. Harder to debug what went wrong. Pricing is silly. Totally makes sense, right? As a user, I really don't care. Okay. It also makes sense. AllLMs can be magic black boxes and I only care about the end result itself, not the end path. It'll be interesting to see how this progresses. A great take. Definitely more nuanced, right? It's not just as straightforward as this is a good thing or a bad thing, but it is true. You know, as a developer, we would like to see into what's going on under the hood. How can I change my inputs to get different outputs? But, you know, this definitely makes it harder. You know, we can't really see into the model. At the same time, this is more my perspective, especially as time goes on, I care more about user output related things, right? As a user, I don't really care. LLMs are already magical black boxes. We only truly care about the end result. I think that's super true. But so, you know, we hop back here. So that's, you know, a great nuanced take. We have a couple more here. And then, of course, at the end here, we have our negative sentiments. We don't have to go through all these, you know, go on Hacker News, read this yourself, digest it. This is just a really interesting use case for, you know, these o1 reasoning models. Again, just really, really kind of gnarly that it was able to do everything I'm asking for here then create a markdown format version of it right really really powerful stuff let's let's wrap up let's talk about some ending thoughts here let's talk about where this is going what we're going to do next here on the channel based on this model and this new paradigm of models so like I mentioned, if I had to really explain the value proposition in one sentence, it's that the O-1 reasoning models are profoundly exceptional at instruction following and iterating on proposed solutions based on your instructions. So, you know, let me just compress that again for you. The real value here is instruction following plus iteration. And all of the results that they mentioned all the benchmarks, they all back that up. All my time so far has revealed the exact same thing, that the more complex the prompt is, the more steps there are, the better results you're going to get. It is a STEM-focused set of models. It's focused on reasoning. It's focusing on, you know, problem solving. So that's something to take into account. Although I have to say most of the benchmarks and most of the experiments I've run show this model beating out sonnet almost across the board. And I know that they do have a mentioning here of the fact that some people prefer the previous models for writing and editing text. I think this is probably up for debate, honestly, whether this is actually functionally true. This is just preference, okay? I think that the reality is, is that this new reasoning model can perform all these things at an accelerated level because it is thinking step by step. But, you know, more details there, more to kind of work through. I don't know if that's actually true or if I'm just super high on this model right now, right? So, you know, a couple more things to note in terms of upcoming videos, in terms of where the channel is going, it's the same as always. We have been predicting this. We knew that better models were coming out. We've been preparing for the 100X. This is not a 100X, so we're definitely over-prepared. But, you know, we have lots of interesting content coming up. I'm very excited to share with you. We're going to be AI coding with these models. The creator of Ader Paul just put out a brand new benchmark showing that the king has been usurped. o1 preview does actually beat out Claude 3.5 Sonnet. Of course, it's not functionally going to be better. It's one of those things where in theory it's better, but based on the speed, no one's going to be firing off1 prompts as much as they are cloud 3.5 sonnet prompts right so we're getting there this is a massive improvement there are many tricks that we can employ to push the performance of o1 even further as we mentioned in previous video hint hint prompt chaining is still a great technique to use so they do say to avoid chain of thought prompts but they don't ever mention here prompt chaining and as we've explored on a channel prompt chaining only gets better in scales with these new models so you can imagine we have three uh oh one mini or three oh one preview calls chained together so these are some kind of you know deeper state bleeding edge ideas that we'll be exploring on the channel there's also the insane epic idea of the fusion chain um if you know you know we'll talk about that more in future videos so a couple interesting things interesting things to mention here about pricing. If you look at Claw 3 Opus, we have 15 in, 75 out per million. The new o1 models, the new O1 preview top of the line, is actually cheaper than Claude 3 Opus. So even though it seems expensive, remember what Opus was like. It's a great model, but it is not doing anything like o1 preview is. It is nowhere near as great as o1 preview. Things are still getting cheaper, which reiterates one of the themes we have been holding onto over the course of this channel. So years now, the price of LLMs is still going to zero. This is a consistent trend we're seeing, even though this is a marked-up model and you might be more hesitant to use this, keep in mind this is the first version of this. Of course, it's the expensive version of this. Open AI and other providers are going to keep pushing these costs down. There are a lot of cases and a lot of prompts that we ran today that o1 Mini would have performed just as well as o1 preview. But, you know, as we're testing out these models, I highly recommend you focus on the state of the art, see what's really possible, and then downsize and save, you know, some resources after you know that many can perform on the same level as a one preview. So Open AI has played their hand. Now it's time for Anthropic to drop the new 3.5 Opus, 3.5 hacud. It's time for Google to, you know, drop Gemini 1.5 Ultra or 2. It's time to see what the other players do. So, you know, in summary, these new reasoning models open up a whole new world of possibilities for generative AI and for you and I as engineers with our boots on the ground, building tools, building products, and building valuable software for ourselves and our users. If you're not subscribed and you made it this far on the video, drop the sub, drop the like, drop a comment, let me know what you think about these models. Do you see the value proposition in these models that I see? Do you see something else? Are there some aspects of these models that you think I missed? Drop a comment down below. Whatever comes next, we'll cover it here on the channel with useful in-depth guides like this where we really find the value of it all. Thanks for watching. Keep building, stay focused, and I'll see you in the next one.